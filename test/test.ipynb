{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('omw-1.4')\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "# def get_synonyms(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     synonyms = []\n",
    "#     for word in tokens:\n",
    "#         for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "#             for lemma in syn.lemmas(lang=\"por\"):\n",
    "#                 synonyms.append(lemma.name())\n",
    "#     return synonyms\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = remove_num(text)\n",
    "    # text = extract_keywords(text)\n",
    "    # text = preprocess_lemma(text)\n",
    "    text = remove_accent(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def arquivo(path,lista):\n",
    "    content = []\n",
    "    with open(path,'r', encoding='utf-8') as file:\n",
    "        linhas = file.readlines()\n",
    "        linhas = [linha.strip() for linha in linhas]\n",
    "    with open(path,'w', encoding='utf-8') as file:\n",
    "        linhas += lista\n",
    "        for j in linhas:\n",
    "            j = preprocess(j)\n",
    "            j = j.lower().strip()\n",
    "            if j not in content and len(j) > 3:\n",
    "                content.append(j)\n",
    "                \n",
    "            j_keyword = extract_keywords(j)\n",
    "            if j_keyword not in content and len(j) > 3:\n",
    "                content.append(j_keyword)\n",
    "            j_lemma = preprocess_lemma(j)\n",
    "            if j_lemma not in content:\n",
    "                content.append(j_lemma)\n",
    "            j_stem = preprocess_stem(j)\n",
    "            if j_stem not in content:\n",
    "                content.append(j_stem)\n",
    "\n",
    "            j_keyword2 = extract_keywords(j_lemma)\n",
    "            if j_keyword2 not in content:\n",
    "                content.append(j_keyword2)\n",
    "            j_stem2 = preprocess_stem(j_lemma)\n",
    "            if j_stem2 not in content:\n",
    "                content.append(j_stem2)\n",
    "\n",
    "            j_lemma3 = preprocess_lemma(j_keyword)\n",
    "            if j_lemma3 not in content:\n",
    "                content.append(j_lemma3)\n",
    "            j_stem3 = preprocess_stem(j_keyword)\n",
    "            if j_stem3 not in content:\n",
    "                content.append(j_stem3)\n",
    "\n",
    "\n",
    "        for i in content:\n",
    "            i = i.strip()\n",
    "\n",
    "            file.writelines(i + '\\n')\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path,'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json('intents_samples2.json')\n",
    "# for i in data:\n",
    "    # if 'INSATISFACAO' in i.keys():\n",
    "    #     lista = i['INSATISFACAO']\n",
    "    # if 'Reclamação' in list(i.values())[0]:\n",
    "    #     lista = i['keywords']\n",
    "arquivo(r'..\\database\\offensive\\OFFENSIVE.txt',lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "result = docx2txt.process(r'..\\censura pt-BR.docx')\n",
    "result = result.split('\\n')\n",
    "\n",
    "lista = []\n",
    "for i in result:\n",
    "    if i != '':\n",
    "        i = i.lower().strip()\n",
    "        lista.append(i)\n",
    "lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from random import choice\n",
    "\n",
    "def load_txt(path):\n",
    "    with open(path,'r', encoding='utf-8') as file:\n",
    "        linhas = file.readlines()\n",
    "        lines = []\n",
    "        for linha in linhas:\n",
    "            linha = linha.strip()\n",
    "            if linha != '':\n",
    "                lines.append(linha)\n",
    "    return lines\n",
    "\n",
    "def sintetizar_dados(lista):\n",
    "    def substituir_sinonimos(frase):\n",
    "        tokens = nltk.word_tokenize(frase)\n",
    "        sinonimos = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sinonimo = choice(wordnet.synsets(token)).lemmas()[0].name()\n",
    "                sinonimos.append(sinonimo if sinonimo != token else token)\n",
    "            except IndexError:\n",
    "                sinonimos.append(token)\n",
    "        return ' '.join(sinonimos)\n",
    "    def rearranjar_estrutura(frase):\n",
    "        tokens = nltk.word_tokenize(frase)\n",
    "        rearranjada = ' '.join(tokens[::-1])\n",
    "        return rearranjada\n",
    "    sintetizados = []\n",
    "    for exemplo in lista:\n",
    "        exemplo_sinonimos = substituir_sinonimos(exemplo)\n",
    "        exemplo_rearranjado = rearranjar_estrutura(exemplo)\n",
    "        sintetizados.extend([exemplo, exemplo_sinonimos, exemplo_rearranjado])\n",
    "    lista2 = []\n",
    "    for i,j in zip(lista,sintetizados):\n",
    "        if i not in lista2:\n",
    "            lista2.append(i)\n",
    "        if j not in lista2:\n",
    "            lista2.append(j)\n",
    "        i2 = extract_keywords(i)\n",
    "        j2 = extract_keywords(j)\n",
    "        if i2 not in lista2:\n",
    "            lista2.append(i2)\n",
    "        if j2 not in lista2:\n",
    "            lista2.append(j2)\n",
    "    lista3 = lista2\n",
    "    for x in lista3:\n",
    "        x = preprocess_lemma(x)\n",
    "        if x not in lista2:\n",
    "            lista2.append(x)\n",
    "    return lista2\n",
    "\n",
    "def salvar_lista_em_arquivo(lista, nome_arquivo):\n",
    "    with open(nome_arquivo, 'a+', encoding=\"utf-8\") as arquivo:\n",
    "        for exemplo in lista:\n",
    "            arquivo.write(exemplo + '\\n')\n",
    "\n",
    "def eliminar_linhas_repetidas(nome_arquivo):\n",
    "    linhas_unicas = set()\n",
    "    with open(nome_arquivo, 'r',encoding=\"utf-8\") as arquivo:\n",
    "        for linha in arquivo:\n",
    "            linha = remove_accent(remove_punct(linha)).strip().lower()\n",
    "            if linha not in linhas_unicas:\n",
    "                linhas_unicas.add(linha)\n",
    "    with open(nome_arquivo, 'w',encoding=\"utf-8\") as arquivo:\n",
    "        for linha in linhas_unicas:\n",
    "            arquivo.write(linha + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = load_txt(r'..\\database\\intents\\intents txt\\FEEDBACK_NEGATIVO.txt')\n",
    "sintetizados = sintetizar_dados(lines)\n",
    "salvar_lista_em_arquivo(sintetizados,r'..\\database\\intents\\intents txt\\FEEDBACK_NEGATIVO.txt')\n",
    "eliminar_linhas_repetidas(r'..\\database\\intents\\intents txt\\FEEDBACK_NEGATIVO.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
