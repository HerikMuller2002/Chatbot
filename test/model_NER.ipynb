{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from re import compile, findall, escape\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n",
    "\n",
    "def preprocess(text, tipo=None):\n",
    "    text = remove_punct(text)\n",
    "    text = remove_num(text)\n",
    "    text = extract_keywords(text)\n",
    "    if tipo == 'lemma':\n",
    "        text = preprocess_lemma(text)\n",
    "    elif tipo == 'stem':\n",
    "        text = preprocess_stem(text)\n",
    "    else:\n",
    "        pass\n",
    "    text = remove_accent(text)\n",
    "    return text\n",
    "\n",
    "def load_txt(path):\n",
    "    with open(path,'r', encoding='utf-8') as file:\n",
    "        linhas = file.readlines()\n",
    "        lines = []\n",
    "        for linha in linhas:\n",
    "            linha = linha.strip()\n",
    "            if linha != '':\n",
    "                lines.append(linha)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    if os.path.isfile(json_path):\n",
    "        with open(json_path, 'r+', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data.append(content)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f, indent=4)\n",
    "    else:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            data = [content]\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def check_intervals(lst):\n",
    "    spans = []\n",
    "    for current_interval in lst:\n",
    "        start, end, label = current_interval\n",
    "        current_span = (start, end, label)\n",
    "        overlap = False\n",
    "        for existing_span in spans:\n",
    "            if existing_span[0] <= start < existing_span[1] or existing_span[0] < end <= existing_span[1]:\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            spans.append(current_span)\n",
    "    corrected_list = [[start, end, label] for start, end, label in spans]\n",
    "    return corrected_list\n",
    "\n",
    "def find_words(text, find_tokens):\n",
    "    result = []\n",
    "    for token in find_tokens:\n",
    "        pattern = compile(r'\\b{}\\b'.format(escape(token)))\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            dictionary = {\n",
    "                \"text\": token,\n",
    "                \"start_index\": match.start(),\n",
    "                \"end_index\": match.end() - 1,\n",
    "                \"start_position\": len(findall(r'\\b\\w+\\b', text[:match.start()])),\n",
    "                \"end_position\": (len(findall(r'\\b\\w+\\b', text[:match.start()])) + len(token.split())) - 1\n",
    "            }\n",
    "            result.append(dictionary)\n",
    "    return result\n",
    "\n",
    "def preparing_data(df, column_label, column_keyword, path, labels_with_texts):\n",
    "    words = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_words = re.split(r',|;|.', row[column_keyword])\n",
    "        for i in range(len(list_words)):\n",
    "            list_words[i] = preprocess(list_words[i],'lemma').strip()\n",
    "        words.append(list_words)\n",
    "    dict_train = {}\n",
    "    for i in range(len(labels_with_texts)):\n",
    "        list_tuple = []\n",
    "        for text in labels_with_texts[i][\"texts\"]:\n",
    "            text = preprocess(text).strip()\n",
    "            list_find_word = find_words(text,words[i])\n",
    "            list_find = []\n",
    "            for j in list_find_word:\n",
    "                list_word_found = [j['start_index'],j['end_index']+1,str(labels_with_texts[i][column_label])]\n",
    "                list_find.append(list_word_found)\n",
    "            list_tuple.append((text, {\"entities\": list_find}))\n",
    "        dict_train[str(labels_with_texts[i][column_label])] = list_tuple\n",
    "\n",
    "        for values in dict_train.values():\n",
    "            for items in values:\n",
    "                for inner_values in items[1].values():\n",
    "                    new_value = check_intervals(inner_values)\n",
    "                    items[1]['entities'] = new_value\n",
    "\n",
    "        create_json(path, dict_train)\n",
    "\n",
    "\n",
    "\n",
    "def criar_dataframe(classe, lista):\n",
    "    data = {'labels': [classe] * len(lista),\n",
    "            'samples': lista}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def concat_df(list_df):\n",
    "    df_concated = pd.concat(list_df, axis=0)\n",
    "    return df_concated\n",
    "\n",
    "def preparing_data(path):\n",
    "    list_df = []\n",
    "    count = 0\n",
    "    for nome_arquivo in os.listdir(path):\n",
    "        file = f'{path}\\{nome_arquivo}'\n",
    "        lines = load_txt(file)\n",
    "        if count < 10:\n",
    "            label = 'bombas'\n",
    "        elif count > 9 and count < 19:\n",
    "            label = 'rolamentos'\n",
    "        elif count > 18 and count < 31:\n",
    "            label = 'válvulas'\n",
    "        elif count > 30 and count < 43:\n",
    "            label = 'acionamentos por corrente'\n",
    "        elif count > 42 and count < 46:\n",
    "            label = 'caixas de engrenagens'\n",
    "        elif count > 45 and count < 54:\n",
    "            label = 'Sistemas de óleo lubrificante'\n",
    "        elif count > 53 and count < 70:\n",
    "            label = 'Acionamentos por correia em V'\n",
    "        elif count > 69 and count < 74:\n",
    "            label = 'Sistemas de ventiladores'\n",
    "        elif count > 73 and count < 79:\n",
    "            label = 'Purgadores de vapor'\n",
    "        elif count > 78 and count < 105:\n",
    "            label = 'Motores elétricos'\n",
    "        elif count > 104 and count < 107:\n",
    "            label = 'Contatos elétricos'\n",
    "        elif count > 106 and count < 110:\n",
    "            label = 'Disjuntores elétricos de caixa moldada'\n",
    "        elif count > 109 and count < 113:\n",
    "            label = 'Circuito magnético'\n",
    "        elif count > 112 and count < 116:\n",
    "            label = 'Circuito dielétrico'\n",
    "        count += 1\n",
    "        df = criar_dataframe(label,lines)\n",
    "        list_df.append(df)\n",
    "    new_df = concat_df(list_df)\n",
    "    return new_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bombas</td>\n",
       "      <td>A bomba está superaquecendo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bombas</td>\n",
       "      <td>A bomba apresenta temperatura elevada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bombas</td>\n",
       "      <td>A bomba está esquentando demais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bombas</td>\n",
       "      <td>O motor da bomba está superaquecido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bombas</td>\n",
       "      <td>A bomba está piscando e emitindo calor excessivo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Motor elétrico não entra em operação após ser ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Falha no arranque do motor elétrico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Motor não liga mesmo após receber energia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Dificuldade em acionar o motor elétrico para a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Motor elétrico não inicia a rotação após ser l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24557 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  labels                                            samples\n",
       "0                 bombas                       A bomba está superaquecendo.\n",
       "1                 bombas             A bomba apresenta temperatura elevada.\n",
       "2                 bombas                   A bomba está esquentando demais.\n",
       "3                 bombas               O motor da bomba está superaquecido.\n",
       "4                 bombas  A bomba está piscando e emitindo calor excessivo.\n",
       "..                   ...                                                ...\n",
       "195  Circuito dielétrico  Motor elétrico não entra em operação após ser ...\n",
       "196  Circuito dielétrico                Falha no arranque do motor elétrico\n",
       "197  Circuito dielétrico          Motor não liga mesmo após receber energia\n",
       "198  Circuito dielétrico  Dificuldade em acionar o motor elétrico para a...\n",
       "199  Circuito dielétrico  Motor elétrico não inicia a rotação após ser l...\n",
       "\n",
       "[24557 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = preparing_data(r'..\\database\\problems\\problemas txt')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Semeq\\Desktop\\Chatbot\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.5.0/pt_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from pt-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (57.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.1.3)\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download \"pt_core_news_sm\"\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import blank, training, load\n",
    "from pathlib import Path\n",
    "import random\n",
    "nlp = load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path_model):\n",
    "    path_model = Path(path_model)\n",
    "    if not path_model.exists():\n",
    "        path_model.mkdir()\n",
    "    model_path = path_model\n",
    "    model.to_disk(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "def train_model(data_dict, epochs, path_model):\n",
    "    nlp = blank(\"pt\")\n",
    "    nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "    for label in data_dict.keys():\n",
    "        nlp.get_pipe(\"ner\").add_label(label)\n",
    "    train_data = []\n",
    "    for label, examples in data_dict.items():\n",
    "        for text, annotations in examples:\n",
    "            train_data.append((text, annotations))\n",
    "    nlp.begin_training()\n",
    "    for itn in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, losses=losses)\n",
    "        print(\"Epoch:\", itn+1, \"Loss:\", losses)\n",
    "    save_model(nlp, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
