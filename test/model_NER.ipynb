{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from re import compile, findall, escape\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "# def preprocess_stem(text):\n",
    "#     stemmer = SnowballStemmer(\"portuguese\")\n",
    "#     tokens = word_tokenize(text)\n",
    "#     stems = []\n",
    "#     for token in tokens:\n",
    "#         stems.append(stemmer.stem(token))\n",
    "#     stems = ' '.join(stems)\n",
    "#     return stems\n",
    "\n",
    "def preprocess(text, tipo=None):\n",
    "    text = remove_punct(text)\n",
    "    text = remove_num(text)\n",
    "    text = extract_keywords(text)\n",
    "    if tipo == 'lemma':\n",
    "        text = preprocess_lemma(text)\n",
    "    elif tipo == 'stem':\n",
    "        text = preprocess_stem(text)\n",
    "    else:\n",
    "        pass\n",
    "    text = remove_accent(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    if os.path.isfile(json_path):\n",
    "        with open(json_path, 'r+', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data.append(content)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f, indent=4)\n",
    "    else:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            data = [content]\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def check_intervals(lst):\n",
    "    spans = []\n",
    "    for current_interval in lst:\n",
    "        start, end, label = current_interval\n",
    "        current_span = (start, end, label)\n",
    "        overlap = False\n",
    "        for existing_span in spans:\n",
    "            if existing_span[0] <= start < existing_span[1] or existing_span[0] < end <= existing_span[1]:\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            spans.append(current_span)\n",
    "    corrected_list = [[start, end, label] for start, end, label in spans]\n",
    "    return corrected_list\n",
    "\n",
    "def find_words(text, find_tokens):\n",
    "    result = []\n",
    "    for token in find_tokens:\n",
    "        pattern = compile(r'\\b{}\\b'.format(escape(token)))\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            dictionary = {\n",
    "                \"text\": token,\n",
    "                \"start_index\": match.start(),\n",
    "                \"end_index\": match.end() - 1,\n",
    "                \"start_position\": len(findall(r'\\b\\w+\\b', text[:match.start()])),\n",
    "                \"end_position\": (len(findall(r'\\b\\w+\\b', text[:match.start()])) + len(token.split())) - 1\n",
    "            }\n",
    "            result.append(dictionary)\n",
    "    return result\n",
    "\n",
    "def preparing_data(df, column_label, column_keyword, path, labels_with_texts):\n",
    "    words = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_words = re.split(r',|;|.', row[column_keyword])\n",
    "        for i in range(len(list_words)):\n",
    "            list_words[i] = preprocess(list_words[i],'lemma').strip()\n",
    "        words.append(list_words)\n",
    "    dict_train = {}\n",
    "    for i in range(len(labels_with_texts)):\n",
    "        list_tuple = []\n",
    "        for text in labels_with_texts[i][\"texts\"]:\n",
    "            text = preprocess(text).strip()\n",
    "            list_find_word = find_words(text,words[i])\n",
    "            list_find = []\n",
    "            for j in list_find_word:\n",
    "                list_word_found = [j['start_index'],j['end_index']+1,str(labels_with_texts[i][column_label])]\n",
    "                list_find.append(list_word_found)\n",
    "            list_tuple.append((text, {\"entities\": list_find}))\n",
    "        dict_train[str(labels_with_texts[i][column_label])] = list_tuple\n",
    "\n",
    "        for values in dict_train.values():\n",
    "            for items in values:\n",
    "                for inner_values in items[1].values():\n",
    "                    new_value = check_intervals(inner_values)\n",
    "                    items[1]['entities'] = new_value\n",
    "\n",
    "        create_json(path, dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Semeq\\Desktop\\Chatbot\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.5.0/pt_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from pt-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (57.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\semeq\\desktop\\chatbot\\.venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.1.3)\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download \"pt_core_news_sm\"\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import blank, training, load\n",
    "from pathlib import Path\n",
    "import random\n",
    "nlp = load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path_model):\n",
    "    path_model = Path(path_model)\n",
    "    if not path_model.exists():\n",
    "        path_model.mkdir()\n",
    "    model_path = path_model\n",
    "    model.to_disk(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "def train_model(data_dict, epochs, path_model):\n",
    "    nlp = blank(\"pt\")\n",
    "    nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "    for label in data_dict.keys():\n",
    "        nlp.get_pipe(\"ner\").add_label(label)\n",
    "    train_data = []\n",
    "    for label, examples in data_dict.items():\n",
    "        for text, annotations in examples:\n",
    "            train_data.append((text, annotations))\n",
    "    nlp.begin_training()\n",
    "    for itn in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, losses=losses)\n",
    "        print(\"Epoch:\", itn+1, \"Loss:\", losses)\n",
    "    save_model(nlp, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
