{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from re import compile, findall, escape\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n",
    "\n",
    "def preprocess(text, tipo=None):\n",
    "    text = remove_punct(text)\n",
    "    text = remove_num(text)\n",
    "    text = extract_keywords(text)\n",
    "    if tipo == 'lemma':\n",
    "        text = preprocess_lemma(text)\n",
    "    elif tipo == 'stem':\n",
    "        text = preprocess_stem(text)\n",
    "    else:\n",
    "        pass\n",
    "    text = remove_accent(text)\n",
    "    return text\n",
    "\n",
    "def load_txt(path):\n",
    "    with open(path,'r', encoding='utf-8') as file:\n",
    "        linhas = file.readlines()\n",
    "        lines = []\n",
    "        for linha in linhas:\n",
    "            linha = linha.strip()\n",
    "            if linha != '':\n",
    "                lines.append(linha)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    if os.path.isfile(json_path):\n",
    "        with open(json_path, 'r+', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data.append(content)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f, indent=4)\n",
    "    else:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            data = [content]\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def check_intervals(lst):\n",
    "    spans = []\n",
    "    for current_interval in lst:\n",
    "        start, end, label = current_interval\n",
    "        current_span = (start, end, label)\n",
    "        overlap = False\n",
    "        for existing_span in spans:\n",
    "            if existing_span[0] <= start < existing_span[1] or existing_span[0] < end <= existing_span[1]:\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            spans.append(current_span)\n",
    "    corrected_list = [[start, end, label] for start, end, label in spans]\n",
    "    return corrected_list\n",
    "\n",
    "def find_words(text, find_tokens):\n",
    "    result = []\n",
    "    for token in find_tokens:\n",
    "        pattern = compile(r'\\b{}\\b'.format(escape(token)))\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            dictionary = {\n",
    "                \"text\": token,\n",
    "                \"start_index\": match.start(),\n",
    "                \"end_index\": match.end() - 1,\n",
    "                \"start_position\": len(findall(r'\\b\\w+\\b', text[:match.start()])),\n",
    "                \"end_position\": (len(findall(r'\\b\\w+\\b', text[:match.start()])) + len(token.split())) - 1\n",
    "            }\n",
    "            result.append(dictionary)\n",
    "    return result\n",
    "\n",
    "def preparing_data(df, keywords, path, labels_with_texts):\n",
    "    list_words = []\n",
    "    for key in keywords:\n",
    "        words = list(set([preprocess(i, 'lemma') for i in key['keywords'] if i != '']))\n",
    "        list_words.append(words)\n",
    "        dict_train = {}\n",
    "    for i in range(len(labels_with_texts)):\n",
    "        list_tuple = []\n",
    "        for text in labels_with_texts[i][\"texts\"]:\n",
    "            for id, row in df.iterrows():\n",
    "                if labels_with_texts[i][\"classe\"] == row['classe']:\n",
    "                    label = row['equipament'].upper()\n",
    "            text = preprocess(text).strip()\n",
    "            list_find_word = find_words(text,list_words[i])\n",
    "            list_find = []\n",
    "            for j in list_find_word:\n",
    "                if j['text'] != '':\n",
    "                    list_word_found = [j['start_index'],j['end_index']+1,label]\n",
    "                    list_find.append(list_word_found)\n",
    "            list_tuple.append((text, {\"entities\": list_find}))\n",
    "        dict_train[label] = list_tuple\n",
    "        for values in dict_train.values():\n",
    "            for items in values:\n",
    "                for inner_values in items[1].values():\n",
    "                    new_value = check_intervals(inner_values)\n",
    "                    items[1]['entities'] = new_value\n",
    "        create_json(path, dict_train)\n",
    "\n",
    "\n",
    "# def preparing_data(df, keywords, path, labels_with_texts):\n",
    "#     for key in keywords:\n",
    "#         words = list(set([preprocess(i, 'lemma') for i in key['keywords'] if i != '']))\n",
    "#     dict_train = {}\n",
    "#     for i in range(len(labels_with_texts)):\n",
    "#         list_tuple = []\n",
    "#         for text in labels_with_texts[i][\"texts\"]:\n",
    "#             text = preprocess(text).strip()\n",
    "#             list_find_word = find_words(text,words[i])\n",
    "#             list_find = []\n",
    "#             for j in list_find_word:\n",
    "#                 list_word_found = [j['start_index'],j['end_index']+1,label]\n",
    "#                 list_find.append(list_word_found)\n",
    "#             list_tuple.append((text, {\"entities\": list_find}))\n",
    "#         dict_train[label] = list_tuple\n",
    "\n",
    "#         for values in dict_train.values():\n",
    "#             for items in values:\n",
    "#                 for inner_values in items[1].values():\n",
    "#                     new_value = check_intervals(inner_values)\n",
    "#                     items[1]['entities'] = new_value\n",
    "\n",
    "#         create_json(path, dict_train)\n",
    "\n",
    "\n",
    "    # dict_train = {}\n",
    "    # for idx, equipament in enumerate(labels_with_texts['labels'].unique()):\n",
    "    #     dict_train[equipament] = []\n",
    "    #     df_filter = labels_with_texts.loc[labels_with_texts['labels'] == equipament]\n",
    "    #     for id, row in df_filter.iterrows():\n",
    "    #         label = row['labels'].upper()\n",
    "    #         text = preprocess(row['samples'], 'lemma').strip()\n",
    "    #         list_find_word = find_words(text, words[idx])\n",
    "    #         if list_find_word:\n",
    "    #             list_find = []\n",
    "    #             for j in list_find_word:\n",
    "    #                 list_word_found = [j['start_index'], j['end_index'] + 1, label]\n",
    "    #                 list_find.append(list_word_found)\n",
    "    #             dict_train[equipament].append((text, {\"entities\": list_find}))\n",
    "    # for values in dict_train.values():\n",
    "    #     for items in values:\n",
    "    #         for inner_values in items[1].values():\n",
    "    #             new_value = check_intervals(inner_values)\n",
    "    #             items[1]['entities'] = new_value\n",
    "    # create_json(path, dict_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def criar_dataframe(classe, lista):\n",
    "    data = {'labels': [classe] * len(lista),\n",
    "            'samples': lista}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def concat_df(list_df):\n",
    "    df_concated = pd.concat(list_df, axis=0)\n",
    "    return df_concated\n",
    "\n",
    "def preparing_data2(path):\n",
    "    list_df = []\n",
    "    count = 0\n",
    "    for nome_arquivo in os.listdir(path):\n",
    "        file = f'{path}\\{nome_arquivo}'\n",
    "        lines = load_txt(file)\n",
    "        if count < 10:\n",
    "            label = 'bombas'\n",
    "        elif count > 9 and count < 19:\n",
    "            label = 'rolamentos'\n",
    "        elif count > 18 and count < 31:\n",
    "            label = 'válvulas'\n",
    "        elif count > 30 and count < 43:\n",
    "            label = 'acionamentos por corrente'\n",
    "        elif count > 42 and count < 46:\n",
    "            label = 'caixas de engrenagens'\n",
    "        elif count > 45 and count < 54:\n",
    "            label = 'Sistemas de óleo lubrificante'\n",
    "        elif count > 53 and count < 70:\n",
    "            label = 'Acionamentos por correia em V'\n",
    "        elif count > 69 and count < 74:\n",
    "            label = 'Sistemas de ventiladores'\n",
    "        elif count > 73 and count < 79:\n",
    "            label = 'Purgadores de vapor'\n",
    "        elif count > 78 and count < 105:\n",
    "            label = 'Motores elétricos'\n",
    "        elif count > 104 and count < 107:\n",
    "            label = 'Contatos elétricos'\n",
    "        elif count > 106 and count < 110:\n",
    "            label = 'Disjuntores elétricos de caixa moldada'\n",
    "        elif count > 109 and count < 113:\n",
    "            label = 'Circuito magnético'\n",
    "        elif count > 112 and count < 116:\n",
    "            label = 'Circuito dielétrico'\n",
    "        count += 1\n",
    "        df = criar_dataframe(label,lines)\n",
    "        list_df.append(df)\n",
    "    new_df = concat_df(list_df)\n",
    "    return new_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe</th>\n",
       "      <th>equipament</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bombas</td>\n",
       "      <td>bomba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rolamentos</td>\n",
       "      <td>rolamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>válvulas</td>\n",
       "      <td>válvula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>acionamentos por corrente</td>\n",
       "      <td>corrente, acionamento corrente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>caixas de engrenagens</td>\n",
       "      <td>caixa engrenagem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Sistemas de óleo lubrificante</td>\n",
       "      <td>reservatório óleo, bomba óleo, filtro óleo, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Acionamentos por correia em V</td>\n",
       "      <td>polias V, correia V, tensionadores V, acoplame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Sistemas de ventiladores</td>\n",
       "      <td>ventilador, exaustor, soprador, sistema ventil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Purgadores de vapor</td>\n",
       "      <td>Purgadores vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Motores elétricos</td>\n",
       "      <td>Motores elétricos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Contatos elétricos</td>\n",
       "      <td>Interruptor, Relé, Contator, Conector, Chave s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Disjuntores elétricos de caixa moldada</td>\n",
       "      <td>Disjuntor, caixa moldada, disjuntores elétricos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Circuito magnético</td>\n",
       "      <td>Transformador, Indutor, Bobina ignição, Núcleo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Circuito dielétrico</td>\n",
       "      <td>Capacitor, Isolador elétrico, Cabo coaxial, Fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    classe                              equipament  \\\n",
       "0        1                                  bombas   \n",
       "1        2                              rolamentos   \n",
       "2        3                                válvulas   \n",
       "3        4               acionamentos por corrente   \n",
       "4        5                   caixas de engrenagens   \n",
       "5        6           Sistemas de óleo lubrificante   \n",
       "6        7           Acionamentos por correia em V   \n",
       "7        8                Sistemas de ventiladores   \n",
       "8        9                     Purgadores de vapor   \n",
       "9       10                       Motores elétricos   \n",
       "10      11                      Contatos elétricos   \n",
       "11      12  Disjuntores elétricos de caixa moldada   \n",
       "12      13                      Circuito magnético   \n",
       "13      14                     Circuito dielétrico   \n",
       "\n",
       "                                             keywords  \n",
       "0                                               bomba  \n",
       "1                                           rolamento  \n",
       "2                                             válvula  \n",
       "3                      corrente, acionamento corrente  \n",
       "4                                    caixa engrenagem  \n",
       "5   reservatório óleo, bomba óleo, filtro óleo, ra...  \n",
       "6   polias V, correia V, tensionadores V, acoplame...  \n",
       "7   ventilador, exaustor, soprador, sistema ventil...  \n",
       "8                                    Purgadores vapor  \n",
       "9                                   Motores elétricos  \n",
       "10  Interruptor, Relé, Contator, Conector, Chave s...  \n",
       "11    Disjuntor, caixa moldada, disjuntores elétricos  \n",
       "12  Transformador, Indutor, Bobina ignição, Núcleo...  \n",
       "13  Capacitor, Isolador elétrico, Cabo coaxial, Fi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_labels = pd.read_excel('classes.xlsx')\n",
    "display(df_labels)\n",
    "\n",
    "import json\n",
    "with open(r'keywords_equipaments.json','r',encoding=\"utf-8\") as f:\n",
    "    keywords_equipaments = json.load(f)\n",
    "with open(r'entidades_samples.json','r',encoding=\"utf-8\") as file:\n",
    "    samples = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing_data(df_labels, keywords_equipaments, 'json_train_NER.json', samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.5.0/pt_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from pt-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (57.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\herik\\onedrive\\área de trabalho\\chatbot\\.venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.1.3)\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\herik\\OneDrive\\Área de Trabalho\\chatbot\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download \"pt_core_news_sm\"\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import blank, training, load\n",
    "from pathlib import Path\n",
    "import random\n",
    "nlp = load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model, path_model):\n",
    "#     path_model = Path(path_model)\n",
    "#     if not path_model.exists():\n",
    "#         path_model.mkdir()\n",
    "#     model_path = path_model\n",
    "#     model.to_disk(model_path)\n",
    "#     print(\"Model saved to:\", model_path)\n",
    "\n",
    "# def train_model(data_dict, epochs, path_model):\n",
    "#     nlp = blank(\"pt\")\n",
    "#     nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "#     data = data_dict[0]\n",
    "#     for label in data.keys():\n",
    "#         nlp.get_pipe(\"ner\").add_label(label)\n",
    "#     train_data = []\n",
    "#     for label, examples in data.items():\n",
    "#         for text, annotations in examples:\n",
    "#             train_data.append((text, annotations))\n",
    "#     nlp.begin_training()\n",
    "#     for itn in range(epochs):\n",
    "#         random.shuffle(train_data)\n",
    "#         losses = {}\n",
    "#         batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "#         for batch in batches:\n",
    "#             texts, annotations = zip(*batch)\n",
    "#             example_batch = []\n",
    "#             for text, annotation in zip(texts, annotations):\n",
    "#                 doc = nlp.make_doc(text)\n",
    "#                 example = training.example.Example.from_dict(doc, annotation)\n",
    "#                 example_batch.append(example)\n",
    "#             nlp.update(example_batch, losses=losses)\n",
    "#         print(\"Epoch:\", itn+1, \"Loss:\", losses)\n",
    "#     save_model(nlp, path_model)\n",
    "\n",
    "def save_model(nlp, path_model):\n",
    "    path_model = Path(path_model)\n",
    "    if not path_model.exists():\n",
    "        path_model.mkdir()\n",
    "    model_path = str(path_model)\n",
    "    nlp.to_disk(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "\n",
    "def train_model(data_list, epochs, path_model):\n",
    "    nlp = init(\"pt\")\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for data in data_list:\n",
    "        for _, annotations in data:\n",
    "            for ent in annotations:\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    train_data = []\n",
    "    for data in data_list:\n",
    "        for text, annotations in data:\n",
    "            entities = []\n",
    "            for start, end, label in annotations:\n",
    "                entities.append((start, end, label))\n",
    "            train_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, drop=0.5, losses=losses)\n",
    "        print(\"Epoch:\", itn+1, \"Loss:\", losses)\n",
    "\n",
    "    save_model(nlp, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: {'ner': 383.69433764757844}\n",
      "Epoch: 2 Loss: {'ner': 123.6943181494562}\n",
      "Epoch: 3 Loss: {'ner': 38.722802261152765}\n",
      "Epoch: 4 Loss: {'ner': 16.51127979042253}\n",
      "Epoch: 5 Loss: {'ner': 9.055934603092245}\n",
      "Epoch: 6 Loss: {'ner': 5.779707924894765}\n",
      "Epoch: 7 Loss: {'ner': 6.008568786807641}\n",
      "Epoch: 8 Loss: {'ner': 3.345335745332085}\n",
      "Epoch: 9 Loss: {'ner': 47.68549356871944}\n",
      "Epoch: 10 Loss: {'ner': 46.859574104781764}\n",
      "Epoch: 11 Loss: {'ner': 2.0948948507165928}\n",
      "Epoch: 12 Loss: {'ner': 0.06428383184463742}\n",
      "Epoch: 13 Loss: {'ner': 0.006224407583691102}\n",
      "Epoch: 14 Loss: {'ner': 0.00044805220314622987}\n",
      "Epoch: 15 Loss: {'ner': 8.777275355254388e-06}\n",
      "Epoch: 16 Loss: {'ner': 7.452502889684638e-06}\n",
      "Epoch: 17 Loss: {'ner': 1.2241198470858317e-06}\n",
      "Epoch: 18 Loss: {'ner': 7.154116777455006e-07}\n",
      "Epoch: 19 Loss: {'ner': 6.656823267872591e-07}\n",
      "Epoch: 20 Loss: {'ner': 2.407977885694445e-07}\n",
      "Model saved to: model\n"
     ]
    }
   ],
   "source": [
    "with open(r'json_train_NER.json','r',encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "train_model(data,20,'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
