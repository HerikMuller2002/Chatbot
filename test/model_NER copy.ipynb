{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from re import compile, findall, escape\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return ' '.join(keywords)\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def preprocess(text, tipo=None):\n",
    "    text = remove_punct(text)\n",
    "    text = extract_keywords(text)\n",
    "    if tipo == 'lemma':\n",
    "        text = preprocess_lemma(text)\n",
    "    elif tipo == 'stem':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    text = remove_accent(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def merge_dataframes(df, json_list):\n",
    "    list_samples = df['samples'].tolist()\n",
    "    for obj in json_list:\n",
    "        if 'texts' in obj:\n",
    "            list_samples += obj['texts']\n",
    "    new_df = pd.DataFrame({'samples': list_samples})\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def check_intervals(lst):\n",
    "    spans = lst[:]\n",
    "    remove_indices = []\n",
    "    for i in range(len(spans)):\n",
    "        for j in range(i + 1, len(spans)):\n",
    "            if (\n",
    "                spans[j][0] <= spans[i][0] <= spans[j][1]\n",
    "                or spans[j][0] <= spans[i][1] <= spans[j][1]\n",
    "            ):\n",
    "                if (spans[i][1] - spans[i][0]) < (spans[j][1] - spans[j][0]):\n",
    "                    remove_indices.append(i)\n",
    "                else:\n",
    "                    remove_indices.append(j)\n",
    "    corrected_list = [\n",
    "        span for index, span in enumerate(spans) if index not in remove_indices\n",
    "    ]\n",
    "    return corrected_list\n",
    "\n",
    "\n",
    "def find_words(text, find_tokens):\n",
    "    result = []\n",
    "    for token in find_tokens:\n",
    "        pattern = compile(r'\\b{}\\b'.format(escape(token)))\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            dictionary = {\n",
    "                \"text\": token,\n",
    "                \"start_index\": match.start(),\n",
    "                \"end_index\": match.end(),\n",
    "                # \"start_position\": len(findall(r'\\b\\w+\\b', text[:match.start()])),\n",
    "                # \"end_position\": (len(findall(r'\\b\\w+\\b', text[:match.start()])) + len(token.split())) - 1\n",
    "            }\n",
    "            result.append(dictionary)\n",
    "    return result\n",
    "\n",
    "def create_json(json_path, content):\n",
    "    if os.path.isfile(json_path):\n",
    "        with open(json_path, 'r+', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data.append(content)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f)\n",
    "    else:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            data = [content]\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data(df, json_data, path):\n",
    "    for index, row in enumerate(df.iterrows()):\n",
    "        if index > 770 and index < 3368:\n",
    "            continue\n",
    "        text = preprocess(row[1]['samples']).strip()\n",
    "        list_words_found = []\n",
    "        for item in json_data:\n",
    "            label = preprocess(item[\"label\"], 'lemma').upper()\n",
    "            keywords = list(set([preprocess(i, 'lemma') for i in item[\"keywords\"] if i != '']))\n",
    "            found_words = find_words(text, keywords)\n",
    "            if found_words:\n",
    "                for i in found_words:\n",
    "                    i['label'] = label\n",
    "                    list_words_found.append([i['start_index'],i['end_index'],label])\n",
    "        list_check = check_intervals(list_words_found)\n",
    "        if list_check:\n",
    "            tuple_text = (text,list_check)\n",
    "            create_json(path, tuple_text)\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('samples_NER.xlsx')\n",
    "\n",
    "with open(r'keywords_equipaments.json','r',encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "preparing_data(df,json_data,'json_train_NER.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
